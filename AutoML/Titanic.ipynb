{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "from azureml.core import Dataset, ComputeTarget\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.pipeline.steps import AutoMLStep\n",
    "from azureml.core import Experiment \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Workspace.create(name='pipelines', subscription_id='65a1016d-0f67-45d2-b838-b8f373d6d52e', resource_group='laobri-ml')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = Workspace.from_config(auth=InteractiveLoginAuthentication(tenant_id=os.environ[\"AML_TENANT_ID\"]))\n",
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_name = \"cpu-compute3\"\n",
    "if not compute_name in ws.compute_targets :\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_D2_V2\",\n",
    "                                                                min_nodes=0,\n",
    "                                                                max_nodes=1)\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # Show the result\n",
    "    print(compute_target.get_status().serialize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AmlCompute(workspace=Workspace.create(name='pipelines', subscription_id='65a1016d-0f67-45d2-b838-b8f373d6d52e', resource_group='laobri-ml'), name=cpu-compute3, id=/subscriptions/65a1016d-0f67-45d2-b838-b8f373d6d52e/resourceGroups/laobri-ml/providers/Microsoft.MachineLearningServices/workspaces/pipelines/computes/cpu-compute3, type=AmlCompute, provisioning_state=Succeeded, location=westus, tags=None)\n"
     ]
    }
   ],
   "source": [
    "compute = AmlCompute(ws, compute_name)\n",
    "print(compute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute\n",
    "\n",
    "# Use conda_dependencies.yml to create a conda environment in the Docker image for execution\n",
    "aml_run_config.environment.python.user_managed_dependencies = False\n",
    "\n",
    "# Specify CondaDependencies obj, add necessary packages\n",
    "aml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n",
    "    conda_packages=['pandas','scikit-learn', 'pyarrow'], \n",
    "    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n",
    "    pin_sdk_version=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Grab an open dataset and register it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is baseline data. If the `Dataset` does not exist, create and register it. Not a part of the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'titanic_ds' in ws.datasets.keys() :\n",
    "    # create a TabularDataset from Titanic training data\n",
    "    web_paths = ['https://dprepdata.blob.core.windows.net/demo/Titanic.csv',\n",
    "                 'https://dprepdata.blob.core.windows.net/demo/Titanic2.csv']\n",
    "    titanic_ds = Dataset.Tabular.from_delimited_files(path=web_paths)\n",
    "\n",
    "    titanic_ds.register(workspace = ws,\n",
    "                                     name = 'titanic_ds',\n",
    "                                     description = 'new titanic training data',\n",
    "                                     create_new_version = True)\n",
    "\n",
    "titanic_ds = Dataset.get_by_name(ws, 'titanic_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.data.tabular_dataset.TabularDataset"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(titanic_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not 'titanic_files_ds' in ws.datasets.keys() :\n",
    "    # create a TabularDataset from Titanic training data\n",
    "    web_paths = ['https://dprepdata.blob.core.windows.net/demo/Titanic.csv',\n",
    "                 'https://dprepdata.blob.core.windows.net/demo/Titanic2.csv']\n",
    "    titanic_ds = Dataset.File.from_files(path=web_paths)\n",
    "\n",
    "    titanic_ds.register(workspace = ws,\n",
    "                                     name = 'titanic_files_ds',\n",
    "                                     description = 'File Dataset of titanic training data',\n",
    "                                     create_new_version = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch to understand Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = titanic_ds.to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyarrow.Table\n",
       "PassengerId: int64\n",
       "Survived: int64\n",
       "Pclass: int64\n",
       "Name: string\n",
       "Sex: string\n",
       "Age: double\n",
       "SibSp: int64\n",
       "Parch: int64\n",
       "Ticket: string\n",
       "Fare: double\n",
       "Cabin: string\n",
       "Embarked: string"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa_table = pa.Table.from_pandas(df)\n",
    "pa_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pq.write_table(pa_table, \"titanic.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataprep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataprep.py\n",
    "from azureml.core import Run\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import train_test_split\n",
    "import argparse\n",
    "\n",
    "RANDOM_SEED=42\n",
    "\n",
    "def prepare_age(df):\n",
    "    # Fill in missing Age values from distribution of present Age values \n",
    "    mean = df[\"Age\"].mean()\n",
    "    std = df[\"Age\"].std()\n",
    "    is_null = df[\"Age\"].isnull().sum()\n",
    "    # compute enough (== is_null().sum()) random numbers between the mean, std\n",
    "    rand_age = np.random.randint(mean - std, mean + std, size = is_null)\n",
    "    # fill NaN values in Age column with random values generated\n",
    "    age_slice = df[\"Age\"].copy()\n",
    "    age_slice[np.isnan(age_slice)] = rand_age\n",
    "    df[\"Age\"] = age_slice\n",
    "    df[\"Age\"] = df[\"Age\"].astype(int)\n",
    "    \n",
    "    # Quantize age into 5 classes\n",
    "    df['Age_Group'] = pd.qcut(df['Age'],5, labels=False)\n",
    "    df.drop(['Age'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def prepare_fare(df):\n",
    "    df['Fare'].fillna(0, inplace=True)\n",
    "    df['Fare_Group'] = pd.qcut(df['Fare'],5,labels=False)\n",
    "    df.drop(['Fare'], axis=1, inplace=True)\n",
    "    return df \n",
    "\n",
    "def prepare_genders(df):\n",
    "    genders = {\"male\": 0, \"female\": 1, \"unknown\": 2}\n",
    "    df['Sex'] = df['Sex'].map(genders)\n",
    "    df['Sex'].fillna(2, inplace=True)\n",
    "    df['Sex'] = df['Sex'].astype(int)\n",
    "    return df\n",
    "\n",
    "def prepare_embarked(df):\n",
    "    df['Embarked'].replace('', 'U', inplace=True)\n",
    "    df['Embarked'].fillna('U', inplace=True)\n",
    "    ports = {\"S\": 0, \"C\": 1, \"Q\": 2, \"U\": 3}\n",
    "    df['Embarked'] = df['Embarked'].map(ports)\n",
    "    return df\n",
    "    \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_path', dest='train_path', required=True)\n",
    "parser.add_argument('--test_path', dest='test_path', required=True)\n",
    "args = parser.parse_args()\n",
    "    \n",
    "titanic_ds = Run.get_context().input_datasets['titanic_ds']\n",
    "df = titanic_ds.to_pandas_dataframe().drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "df = prepare_embarked(prepare_genders(prepare_fare(prepare_age(df))))\n",
    "\n",
    "#train, test = train_test_split(df, test_size = 0.2)\n",
    "\n",
    "# Write outpout\n",
    "os.makedirs(os.path.dirname(args.train_path), exist_ok=True)\n",
    "#os.makedirs(os.path.dirname(args.test_path), exist_ok=True)\n",
    "\n",
    "pq.write_table(pa.Table.from_pandas(df), args.train_path)\n",
    "#pq.write_table(pa.Table.from_pandas(test), args.test_path)\n",
    "\n",
    "print(f\"Wrote test to {args.train_path} and train to {args.test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = PipelineData(\"titanic_train\", datastore).as_dataset()\n",
    "test_path = PipelineData(\"titanic_test\", datastore).as_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataprep_step = PythonScriptStep(\n",
    "    name=\"dataprep\", \n",
    "    script_name=\"dataprep.py\", \n",
    "    compute_target=compute, \n",
    "    runconfig=aml_run_config,\n",
    "    arguments=[\"--train_path\", train_path, \"--test_path\", test_path],\n",
    "    inputs=[titanic_ds.as_named_input(\"titanic_ds\")],\n",
    "    outputs=[train_path, test_path]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Train with AutoMLStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - The AutoMLConfig inputs you have specified will soon be deprecated. Please use the AutoMLConfig shown in our documentation: https://aka.ms/AutoMLConfig\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML config created.\n"
     ]
    }
   ],
   "source": [
    "# type(train_path) == PipelineOutputFileDataset\n",
    "# type(train_potds) == PipelineOutputTabularDataset\n",
    "train_potds = train_path.parse_parquet_files(file_extension=None)\n",
    "#test_potds = test_path.parse_parquet_files(file_extension=None)\n",
    "\n",
    "X = train_potds.drop_columns('Survived')\n",
    "y = train_potds.keep_columns('Survived')\n",
    "#test_X = test_potds.drop_columns('Survived')\n",
    "#test_y = test_potds.keep_columns('Survived')\n",
    "\n",
    "train_model_folder = './scripts/trainmodel'\n",
    "# Change iterations to a reasonable number (50) to get better accuracy\n",
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\" : 10,\n",
    "    \"iterations\" : 2,\n",
    "    \"primary_metric\" : 'AUC_weighted',\n",
    "    \"n_cross_validations\" : 2\n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automated_ml_errors.log',\n",
    "                             path = train_model_folder,\n",
    "                             compute_target = compute,\n",
    "                             run_configuration = aml_run_config,\n",
    "                             featurization = 'auto',\n",
    "                             X = X,\n",
    "                             y = y,\n",
    "                             #X_valid = test_X,\n",
    "                             #y_valid = test_y,\n",
    "                             **automl_settings)\n",
    "                             \n",
    "print(\"AutoML config created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_step created.\n"
     ]
    }
   ],
   "source": [
    "train_step = AutoMLStep(name='AutoML_Classification',\n",
    "                                 automl_config=automl_config,\n",
    "                                 passthru_automl_config=False,\n",
    "                                 allow_reuse=True)\n",
    "print(\"train_step created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not 'titanic_automl' in ws.experiments.keys() :\n",
    "    Experiment(ws, 'titanic_automl')\n",
    "experiment = ws.experiments['titanic_automl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(ws, [dataprep_step, train_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step dataprep [046e7a47][07e3bc16-c21c-4645-b8fe-20f5c23ff544], (This step will run and generate new outputs)\n",
      "Created step AutoML_Classification [00f1e9cb][783a685e-f34d-4e57-8d11-6068cd20b735], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 9e4baf6c-532f-4b45-8a45-1f276f53b0f7\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/titanic_automl/runs/9e4baf6c-532f-4b45-8a45-1f276f53b0f7?wsid=/subscriptions/65a1016d-0f67-45d2-b838-b8f373d6d52e/resourcegroups/laobri-ml/workspaces/pipelines\n"
     ]
    }
   ],
   "source": [
    "run = experiment.submit(pipeline, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PipelineRunId: 9e4baf6c-532f-4b45-8a45-1f276f53b0f7\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/experiments/titanic_automl/runs/9e4baf6c-532f-4b45-8a45-1f276f53b0f7?wsid=/subscriptions/65a1016d-0f67-45d2-b838-b8f373d6d52e/resourcegroups/laobri-ml/workspaces/pipelines\n",
      "\n",
      "PipelineRun Execution Summary\n",
      "==============================\n",
      "PipelineRun Status: Finished\n",
      "{'runId': '9e4baf6c-532f-4b45-8a45-1f276f53b0f7', 'status': 'Completed', 'startTimeUtc': '2020-04-24T21:53:25.999204Z', 'endTimeUtc': '2020-04-24T22:11:55.315969Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://pipelines8090722083.blob.core.windows.net/azureml/ExperimentRun/dcid.9e4baf6c-532f-4b45-8a45-1f276f53b0f7/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=2%2FwRmhWqeDOTvsnNY97%2FhYRK4yebP7%2BtGO%2FuOkIFnzM%3D&st=2020-04-24T22%3A04%3A25Z&se=2020-04-25T06%3A14%3A25Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://pipelines8090722083.blob.core.windows.net/azureml/ExperimentRun/dcid.9e4baf6c-532f-4b45-8a45-1f276f53b0f7/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=y1lGIi9plVdMdG9UhBO9BpS5DpMSs3oYdD6RFYY%2FP5U%3D&st=2020-04-24T22%3A04%3A25Z&se=2020-04-25T06%3A14%3A25Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://pipelines8090722083.blob.core.windows.net/azureml/ExperimentRun/dcid.9e4baf6c-532f-4b45-8a45-1f276f53b0f7/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=sIFoV1%2BBHJvjwyFpZhgwIhQVepd15gUE7yyOsYlGiWM%3D&st=2020-04-24T22%3A04%3A25Z&se=2020-04-25T06%3A14%3A25Z&sp=r'}}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'9e4baf6c-532f-4b45-8a45-1f276f53b0f7'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion()\n",
    "run.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.pipeline_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorResponseException",
     "evalue": "(BadRequest) PipelineRun output with name default_metrics_AutoML_Classification does not exist.\nParameter name: pipelineRunOutputName",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mErrorResponseException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-c8e29aa020ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipeline_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'default_metrics_AutoML_Classification'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnum_file_downloaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mget_pipeline_output\u001b[0;34m(self, pipeline_output_name)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPortDataReference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \"\"\"\n\u001b[0;32m--> 252\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipeline_run_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipeline_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_output_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_seconds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/pipeline/core/_aeva_provider.py\u001b[0m in \u001b[0;36mget_pipeline_output\u001b[0;34m(self, context, pipeline_run_id, pipeline_output_name)\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpipeline_output_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         \"\"\"\n\u001b[0;32m-> 1058\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_service_caller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipeline_run_output_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_run_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_output_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1059\u001b[0m         data_reference = _AevaPortDataReferenceProvider.get_data_reference_from_output(self._workspace,\n\u001b[1;32m   1060\u001b[0m                                                                                        \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/pipeline/core/_restclients/aeva/service_caller.py\u001b[0m in \u001b[0;36mget_pipeline_run_output_async\u001b[0;34m(self, pipeline_run_id, pipeline_run_output_name)\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0msubscription_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subscription_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresource_group_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource_group_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mworkspace_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workspace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_run_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpipeline_run_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             pipeline_run_output_name=pipeline_run_output_name, custom_headers=self._get_custom_headers())\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/bin/miniconda3/envs/azureml/lib/python3.7/site-packages/azureml/pipeline/core/_restclients/aeva/aml_pipelines_api10.py\u001b[0m in \u001b[0;36mget_pipeline_run_output_async\u001b[0;34m(self, subscription_id, resource_group_name, workspace_name, pipeline_run_id, pipeline_run_output_name, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[1;32m   2440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2442\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mErrorResponseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2444\u001b[0m         \u001b[0mdeserialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mErrorResponseException\u001b[0m: (BadRequest) PipelineRun output with name default_metrics_AutoML_Classification does not exist.\nParameter name: pipelineRunOutputName"
     ]
    }
   ],
   "source": [
    "metrics_output = run.get_pipeline_output('default_metrics_AutoML_Classification')\n",
    "num_file_downloaded = metrics_output.download('.', show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StepRun' object has no attribute 'find_step_run'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-ffffe32b5d47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchild_runs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_step_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AutoML_classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'StepRun' object has no attribute 'find_step_run'"
     ]
    }
   ],
   "source": [
    "child_runs[0].find_step_run(\"AutoML_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
